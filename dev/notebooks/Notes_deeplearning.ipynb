{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eda28eef-8273-4f2c-9627-a94d15320963",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Deeplearning - Datascience "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10abe40b-e29f-4b91-bf74-260d3a862212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notes from the book Understanding Deeplearning\n",
    "https://udlbook.github.io/udlbook/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efd7cdf-90f2-4002-b9ad-9a91d4002ff9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Definities\n",
    "### hidden state or state\n",
    "f: X --> Y \n",
    "Number of layers between input to output \n",
    "### Loss or training error: \n",
    "We quantify the total mismatch, training error, or loss as the sum of the squares of these deviations for all I training pairs:\n",
    "$$ L[ϕ] = \\sum_{i=1}^n(f[x_i,ϕ] − y_i)^2 $$\n",
    "Since the best parameters minimize this expression, we call this a least-squares loss. The squaring operation means that the direction of the deviation (i.e., whether the line is above or below the data) is unimportant.\n",
    "### Loss function or cost function\n",
    "The loss L is a function of the parameters ϕ, ϕ^r is the result of the function;\n",
    "$$ ϕ^r = argmin [L[ϕ]]$$\n",
    "### Training\n",
    "The process of finding parameters that minimize the loss is termed model fitting, training,\n",
    "or learning.\n",
    "### Generative vs. discriminative models: \n",
    "The models y = f[x,ϕ] models. These make an output prediction y from real-world measurements x. Another approach is to build a generative model x = g[y,ϕ], in which the real-world measurements x are computed as a function of the output y.\n",
    "### Rectified linear unit or ReLU:\n",
    "a[z] = ReLU[z] = (0 z < 0 | z z ≥ 0 )\n",
    "This returns the input when it is positive and zero otherwise\n",
    "### Universal approximation theorem\n",
    "The universal approximation theorem proves that for any continuous function, there exists a shallow network that can approximate this function to any specified precision.\n",
    "### Width, depth, capacity of a network\n",
    "The number of **hidden units in each layer** is referred to as the **width** of the network, and the number of **hidden layers** as the **depth**. The total number of hidden units is a measure of the network’s capacity.\n",
    "### Maximum likelihood\n",
    "We now consider the model as computing a conditional probability distribution Pr(y|x) over possible outputs y given input x. The loss encourages each training output yi to have a high probability under the distribution Pr(yi|xi) computed from the corresponding input xi.\n",
    "First, we choose a parametric distribution Pr(y|θ) defined on the output domain y. Then we use the network to compute one or more of the parameters θ of this distribution. For example, suppose the prediction domain is the set of real numbers, so y ∈ R.\n",
    "Here, we might choose the univariate normal distribution, which is defined on R. This distribution is defined by the mean μ and variance σ2, so θ = {μ, σ2}. The machine learning model might predict the mean μ, and the variance σ2 could be treated as an unknown constant. The model now computes different distribution parameters θi = f[xi,ϕ] for each training input xi.The combined probability term is the likelihood of the parameters, and hence equation 5.1 is known as the maximum likelihood criterion\n",
    "### Minimum log-likelihood\n",
    "This log-likelihood criterion is equivalent because the logarithm is a monotonically increasing function: if z > z′, then log[z] > log[z′] and vice versa (figure 5.2). It follows that when we change the model parameters ϕ to improve the log-likelihood criterion, we also improve the original maximum likelihood criterion. It also follows that the overall maxima of the two criteria must be in the same place, so the best model parameters ˆϕ\n",
    "are the same in both cases. Finally, we note that, by convention, model fitting problems are framed in terms of\n",
    "minimizing a loss. To convert the maximum log-likelihood criterion to a minimization problem, we multiply by minus one, which gives us the negative log-likelihood criterion.\n",
    "### Inference\n",
    "The network no longer directly predicts the outputs y but instead determines a probability distribution over y. When we perform inference, we often want a point estimate rather than a distribution, so we return the maximum of the distribution:\n",
    "ˆy = argmax Pr(y|f[x,ˆϕ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e848b000-b02d-4ce3-badb-d6fca780607b",
   "metadata": {},
   "source": [
    "## Regularization techniques\n",
    "### L2 regularization\n",
    "For neural networks, L2 regularization is usually applied to the weights but not the biases and is hence referred to as a weight decay term. The effect is to encourage smaller weights, so the output function is smoother. To see this, consider that the output prediction is a weighted sum of the activations at the last hidden layer. If the weights have a smaller magnitude, the output will vary less. The same logic applies to\n",
    "the computation of the pre-activations at the last hidden layer and so on, progressing backward through the network. In the limit, if we forced all the weights to be zero, the network would produce a constant output determined by the final bias parameter. \n",
    "If the network is overfitting, then adding the regularization term means that the network must trade off slavish adherence to the data against the desire to be smooth. One way to think about this is that the error due to variance reduces (the model no longer needs to pass through every data point) at the cost of increased bias (the model can only describe smooth functions).\n",
    "\n",
    "### Implicit regularization in gradient descent\n",
    "\n",
    "### Implicit regularization in stochastic gradient descent\n",
    "SGD implicitly favors places where the gradients are stable (where all the batches agree on the slope). Once more, this modifies the trajectory of the optimization process (figure 9.4) but does not necessarily change the position of the global minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd831b7-b3fe-45b2-aa07-a9521fdcc563",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "### MSE (regression models):\n",
    "Mean Average Error, does typically not work that well as a loss function. The default for regression is a variation on this, the Mean Squared Error:\n",
    "\n",
    "$$MSE = \\frac{1}{n}\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2$$\n",
    "\n",
    "This is the mean $\\frac{1}{n}\\sum_{i=1}^n$ of the squared error $(Y_i - \\hat{Y}_i)^2$ \n",
    "But torch has already implemented that for us in an optimized way:\n",
    "````\n",
    "loss = torch.nn.MSELoss()\n",
    "loss(yhat, y)\n",
    "````\n",
    "### Negative log likelihood. \n",
    "The function is:\n",
    "\n",
    "$$NLL = - log(\\hat{y}[c])$$\n",
    "\n",
    "Or: take the probabilities $\\hat{y}$, and pick the probability of the correct class $c$ from the list of probabilities with $\\hat{y}[c]$. Now take the log of that.\n",
    "\n",
    "The log has the effect that predicting closer to 0 if it should have been 1 is punished extra.\n",
    "````\n",
    "loss = torch.nn.NLLLoss()\n",
    "loss(yhat, y)\n",
    "````\n",
    "###  Softmax:\n",
    "Will scale everything between 0 and 1 and sum to 1\n",
    "````\n",
    "m = torch.nn.softmax()\n",
    "yhat = m(input)\n",
    "````\n",
    "###  Sigmoid:\n",
    "Will scale everything between 0 and 1, but without making everything sum to 1\n",
    "````\n",
    "m = torch.nn.Sigmoid()\n",
    "yhat = m(input)\n",
    "````\n",
    "### Tanh\n",
    "Scale everything from -1 to 1 y = tanh x\n",
    "Voordeel with respect to ReLU is that derivatives are smaller, Relu produces growing derivatives \n",
    "### Categorical Cross-Entropy Loss  (multiclass)\n",
    "CEL adds the LogSoftmax to the loss. <b>This means you don't need to add a LogSoftmax layer to your model</b>.\n",
    "This is used when the model needs to classify an input into one of many classes. It compares the predicted probability distribution across all classes with the actual one-hot encoded labels.\n",
    "In PyTorch, you can use torch.nn.CrossEntropyLoss, which expects raw logits (not passed through softmax) and will apply softmax internally.\n",
    "\n",
    "### Binary Cross-Entropy Loss (Log Loss)\n",
    "This loss function is used when the output consists of a single probability value representing the likelihood that the input belongs to class 1. The binary cross-entropy loss is also known as log loss.\n",
    "\n",
    "$$ BCE = \\frac{1}{n}\\sum_{i=1}^n y_i \\cdot log(\\hat{y}_i) + (1-y_i) \\cdot log(1-\\hat{y}_i) $$\n",
    "\n",
    "Or, in plain language: \n",
    "- assume that $y$ is a binary label (0 or 1)\n",
    "- predict the probability $\\hat{y}$\n",
    "- if the label is 1, take the log of the probability: $y_i \\cdot log(\\hat{y}_i$)\n",
    "- if the label is 0, take the log of $1-\\hat{y}$\n",
    "- take the mean $\\frac{1}{n}\\sum_{i=1}^n$ of that\n",
    "\n",
    "````\n",
    "loss = torch.nn.BCELoss()\n",
    "loss(yhat, y)\n",
    "````\n",
    "###  Binary Cross Entropy with logits \n",
    "In the case you dont have predictions with values between 0 and 1, you can use the WithLogits variation. You can then skip the sigmoid layer.\n",
    "In PyTorch, you can use torch.nn.BCEWithLogitsLoss for binary classification tasks. This loss function expects raw output logits from the model (not passed through a sigmoid function), and it applies the sigmoid internally.\n",
    "\n",
    "````\n",
    "loss = torch.nn.BCEWithLogitsLoss()\n",
    "loss(input, y)\n",
    "````\n",
    "# Wrapup\n",
    "\n",
    "Losses are very important: they tell your model what is \"right\" and \"wrong\" and determines what the model will learn!\n",
    "\n",
    "- For regression models, typically use a MSE\n",
    "- For classification, use  BinaryCrossEntropy  (note: this might be implemented different in other libraries like Tensorflow!)\n",
    "- For multiclass, use CrossEntropyLoss\n",
    "\n",
    "There are other, more complex losses for more complex usecases but these three will cover 80% of your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4ac209-e186-4214-b923-e81a2cd03a46",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Resnets\n",
    "Learn of residuals (difference between states)\n",
    "\n",
    "### RNN Recurrent Neural Networks\n",
    "RNN zijn ontworpen voor timeseries\n",
    "Activations: Tanh (-1 to 1)\n",
    "1982 discovered by John Hopfield\n",
    "1995 LSTM achitecture\n",
    "2013 LSTM outperforms models Natural language recognition\n",
    "Je kunt een CNN ook gebruiken voor timeseries, door een matrix te gebruiken\n",
    "CNN: 2D tensor (B, D) (HxBxchannel zijn Dimensies)\n",
    "Timeseries: 3D tensor (B, Sequence, D)\n",
    "(batchsize, sqeuence, D=8 metingen)\n",
    "Bij timeseries de context is belangrijk, door context verandert de betekenins\n",
    "Timeseries: volgorde in tijd is belangrijk, hoeveel van het verleden is nodig om een voorspelling te doen (window)\n",
    "hoveel in de toekomst je wilt voorspellen (horizon), zonder data te lekken (normaliseren van data met gemiddelde uit de toekomst is data lekken).\n",
    "RNN have not explicit forget or retain memory.\n",
    "\n",
    "### GRU Gate Residual Unit\n",
    "Gamma = gate\n",
    "Remember the past en completely ingnor the new state (nagate new info)\n",
    "Forget the past and focus on the present or something in between\n",
    "\n",
    "To create a gate: sigmoid activation (between 0 and 1)\n",
    "\n",
    "Remember: multiplicate by 1\n",
    "\n",
    "Forget: multiplicate by 0\n",
    "Gate is een matrix van zelfde omvang met sigmoid (tussen 0 and 1)\n",
    "\n",
    "candidate hidden state = tanh of a matrix\n",
    "\n",
    "gate * hidden state = hidden state with reset or update *is harmand multiplication of matrix (pointwise)\n",
    "\n",
    "full GRU; has two gates, update and reset gate\n",
    "\n",
    "### LSTM\n",
    "3 Gates: in the computation of the new state LSTM uses two gates forget and update states, instead of the single update gate of the GRU \n",
    "\n",
    "input: number of units of hidden state\n",
    "one to many: generate a name with one letter as initial\n",
    "many to one: review --> sentiment\n",
    "many to many: vertalen / give me all verbs in the sentence\n",
    "ontwerpkeuze: smallere units / more layers for ex. 128 units, 3 layers\n",
    "\n",
    "### Naive model\n",
    "Very simple mode, as baseline\n",
    "First use the naive model to do predictions, create a baseline for your error and this is the base line, everything above is not correct\n",
    "For ex. weather of today is the same a weather of tomorrow\n",
    "\n",
    "### Deeplearning in practice\n",
    "\n",
    "1. welke type NN model ga ik gebruiken?\n",
    "2. aantal units en lagen\n",
    "3. activations and optimizers\n",
    "4. learning rate, scheduler, dropout\n",
    "5. batch and regularization\n",
    "The most important is to know, do you need memory in timeseries? --> LSTM/GRU is the best choice (ex. gesture dataset, or language/translation, audio)\n",
    "\n",
    "### Input tensors \n",
    "2D tensor linear\n",
    "3D tensor RNN, GRU, LSTM, CONV1D (timeseries)\n",
    "4D tensor CONV2D MAXPOOL2D, BATCH NORM 2D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b667b2b3-1b7b-41e4-a868-e7d07bea4b12",
   "metadata": {},
   "source": [
    "### TMUX \n",
    "Als je de VM wil laten draaien bijv. s'nachts\n",
    "ctrl + b\n",
    "ctrl loss +b\n",
    "tmux a -t 0 #select process 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa0907b-1f42-46b9-96d4-3a23d479785a",
   "metadata": {},
   "source": [
    "```rye install mlflow```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b9cf6a",
   "metadata": {},
   "source": [
    "Start stop mlflow:\n",
    "```\n",
    "mlflow server     --backend-store-uri sqlite:///mlflow.db     --default-artifact-root ./mlruns     --host 127.0.0.1:5000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce92bd0f-4aa6-479b-a5aa-0d93a963068c",
   "metadata": {},
   "source": [
    "### Curse of dimentionality\n",
    "Search space is the product of all the hyperparameters options, can be huge. To decrease the search space, choose basis defaults for parameters which are not of big influence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a549c82-1b74-4b78-8ffa-7185f327938b",
   "metadata": {},
   "source": [
    "1. Architectuur choice: think about the architecture, intuition\n",
    "2. Ray is agnostic, works for tensorflow and pythorch, works less good with windows\n",
    "3. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cc102a-6a04-4b13-a8e6-5384fec778f4",
   "metadata": {},
   "source": [
    "Baysian search: search for the best solution, by random trying, requires a distribution of you parameters\n",
    "Baysian Hyperband: stops earlier if after x epochs is not good enough, with risk that you miss the right optimization\n",
    "Tree parser: for strange search spaces, works with a tree structure\n",
    "Mostly start with Baysians search or Hyperband, then later tree parsers\n",
    "\n",
    "### Learning rate schedulers\n",
    "LR 0.1 is handy to start high and end low with 0.001-4 (very small), these lr are set in the scheduler\n",
    "Reduce on Plataut: reduce (divide by 10) lr when leaning stops (error reach plateaus, constant error rate), the pacience determine the moment of decrease of lr\n",
    "Cosine Warm-up: vry big datasets: starts with a low lr to start a warming up, to prevent that you set the parameters too early\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aabb2e0-3e73-4e31-8e2b-aae47930d949",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ray Hypertuning\n",
    "Ray -> Keyconcepts -> config (types of options)\n",
    "Hyperband: niet intelligent, random hypertuning\n",
    "\n",
    "layers vs hidden size\n",
    "kies twee parameters\n",
    "opt (Adam, SGD, AdamB vs lr)\n",
    "units vs dropout\n",
    "units vs layers = verhoogt dipte of breedte\n",
    "GRU/LSTM vs layers = verschil tussen GRU en LSTM\n",
    "GrU/LSTM vs window = langere window is meer geheugen, lstm heeft een complexere geheugen, mogelijk met verschillende datasets\n",
    "run hypertune.py\n",
    "analyse van de data kan in 03_ray notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60170aeb-1605-495d-ba1e-f08df2937540",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2326155033.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    features layer = linear layer vervang je met eigen layer (of twee linear layers met dropout)\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## transfer learning\n",
    "param.requires_grad = False # freeze de model grandients zoals ze niet meetrenen\n",
    "\n",
    "features layer = linear layer vervang je met eigen layer (of twee linear layers met dropout)\n",
    "\n",
    "Normalize: gemiddelde standard deviatie van de resnet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd8a1c3-376d-4ede-b682-578a8e8115cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
